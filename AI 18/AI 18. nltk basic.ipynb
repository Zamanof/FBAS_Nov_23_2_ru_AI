{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-10T15:26:33.988876Z",
     "start_time": "2025-12-10T15:26:27.214498Z"
    }
   },
   "source": [
    "from random import sample\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Токенизация",
   "id": "4b9ec97712ed4727"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:18:22.927106Z",
     "start_time": "2025-12-10T16:18:22.918411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"On a rainy Saturday morning, a young developer decided to explore new machine-learning techniques. don't cry. i'm not. zamanov@itstep.org\n",
    "She opened her laptop, brewed some strong coffee, and began reading a long article about natural language processing. To be, or not to be?\n",
    "The article explained how tokens are created, why lemmatization improves accuracy, and when stemming can distort meaning. Bombastic, fantastic! salam@balam.xalam\n",
    "Excited by these ideas, the developer built a small prototype that analyzed movie reviews, predicted sentiments, and even generated short summaries.\n",
    "Although the first results were imperfect, she felt motivated to continue experimenting and improving her model.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Source text\")\n",
    "print(text)\n",
    "print(\"\\nTokens (word)\")\n",
    "print(tokens)"
   ],
   "id": "6c678a2116ceb677",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text\n",
      "On a rainy Saturday morning, a young developer decided to explore new machine-learning techniques. don't cry. i'm not. zamanov@itstep.org\n",
      "She opened her laptop, brewed some strong coffee, and began reading a long article about natural language processing. To be, or not to be?\n",
      "The article explained how tokens are created, why lemmatization improves accuracy, and when stemming can distort meaning. Bombastic, fantastic! salam@balam.xalam\n",
      "Excited by these ideas, the developer built a small prototype that analyzed movie reviews, predicted sentiments, and even generated short summaries.\n",
      "Although the first results were imperfect, she felt motivated to continue experimenting and improving her model.\n",
      "\n",
      "Tokens (word)\n",
      "['On', 'a', 'rainy', 'Saturday', 'morning', ',', 'a', 'young', 'developer', 'decided', 'to', 'explore', 'new', 'machine-learning', 'techniques', '.', 'do', \"n't\", 'cry', '.', 'i', \"'m\", 'not', '.', 'zamanov', '@', 'itstep.org', 'She', 'opened', 'her', 'laptop', ',', 'brewed', 'some', 'strong', 'coffee', ',', 'and', 'began', 'reading', 'a', 'long', 'article', 'about', 'natural', 'language', 'processing', '.', 'To', 'be', ',', 'or', 'not', 'to', 'be', '?', 'The', 'article', 'explained', 'how', 'tokens', 'are', 'created', ',', 'why', 'lemmatization', 'improves', 'accuracy', ',', 'and', 'when', 'stemming', 'can', 'distort', 'meaning', '.', 'Bombastic', ',', 'fantastic', '!', 'salam', '@', 'balam.xalam', 'Excited', 'by', 'these', 'ideas', ',', 'the', 'developer', 'built', 'a', 'small', 'prototype', 'that', 'analyzed', 'movie', 'reviews', ',', 'predicted', 'sentiments', ',', 'and', 'even', 'generated', 'short', 'summaries', '.', 'Although', 'the', 'first', 'results', 'were', 'imperfect', ',', 'she', 'felt', 'motivated', 'to', 'continue', 'experimenting', 'and', 'improving', 'her', 'model', '.']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:18:25.163860Z",
     "start_time": "2025-12-10T16:18:25.156411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}, sentence: {sentence.strip()}\")"
   ],
   "id": "bc7aad221f4b7ec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, sentence: On a rainy Saturday morning, a young developer decided to explore new machine-learning techniques.\n",
      "2, sentence: don't cry.\n",
      "3, sentence: i'm not.\n",
      "4, sentence: zamanov@itstep.org\n",
      "She opened her laptop, brewed some strong coffee, and began reading a long article about natural language processing.\n",
      "5, sentence: To be, or not to be?\n",
      "6, sentence: The article explained how tokens are created, why lemmatization improves accuracy, and when stemming can distort meaning.\n",
      "7, sentence: Bombastic, fantastic!\n",
      "8, sentence: salam@balam.xalam\n",
      "Excited by these ideas, the developer built a small prototype that analyzed movie reviews, predicted sentiments, and even generated short summaries.\n",
      "9, sentence: Although the first results were imperfect, she felt motivated to continue experimenting and improving her model.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:18:25.833718Z",
     "start_time": "2025-12-10T16:18:25.824242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import (TreebankWordTokenizer, WordPunctTokenizer, RegexpTokenizer)\n",
    "\n",
    "treebank = TreebankWordTokenizer()\n",
    "print(\"\\n TreebankWordTokenizer\")\n",
    "print(treebank.tokenize(text))\n",
    "\n",
    "word_punct = WordPunctTokenizer()\n",
    "\n",
    "print(\"\\n WordPunctTokenizer\")\n",
    "print(word_punct.tokenize(text))\n",
    "\n",
    "\n",
    "regex = RegexpTokenizer(r\"[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\")\n",
    "print(\"\\n RegexpTokenizer\")\n",
    "print(regex.tokenize(text))\n"
   ],
   "id": "1e817c9db239bba0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TreebankWordTokenizer\n",
      "['On', 'a', 'rainy', 'Saturday', 'morning', ',', 'a', 'young', 'developer', 'decided', 'to', 'explore', 'new', 'machine-learning', 'techniques.', 'do', \"n't\", 'cry.', 'i', \"'m\", 'not.', 'zamanov', '@', 'itstep.org', 'She', 'opened', 'her', 'laptop', ',', 'brewed', 'some', 'strong', 'coffee', ',', 'and', 'began', 'reading', 'a', 'long', 'article', 'about', 'natural', 'language', 'processing.', 'To', 'be', ',', 'or', 'not', 'to', 'be', '?', 'The', 'article', 'explained', 'how', 'tokens', 'are', 'created', ',', 'why', 'lemmatization', 'improves', 'accuracy', ',', 'and', 'when', 'stemming', 'can', 'distort', 'meaning.', 'Bombastic', ',', 'fantastic', '!', 'salam', '@', 'balam.xalam', 'Excited', 'by', 'these', 'ideas', ',', 'the', 'developer', 'built', 'a', 'small', 'prototype', 'that', 'analyzed', 'movie', 'reviews', ',', 'predicted', 'sentiments', ',', 'and', 'even', 'generated', 'short', 'summaries.', 'Although', 'the', 'first', 'results', 'were', 'imperfect', ',', 'she', 'felt', 'motivated', 'to', 'continue', 'experimenting', 'and', 'improving', 'her', 'model', '.']\n",
      "\n",
      " WordPunctTokenizer\n",
      "['On', 'a', 'rainy', 'Saturday', 'morning', ',', 'a', 'young', 'developer', 'decided', 'to', 'explore', 'new', 'machine', '-', 'learning', 'techniques', '.', 'don', \"'\", 't', 'cry', '.', 'i', \"'\", 'm', 'not', '.', 'zamanov', '@', 'itstep', '.', 'org', 'She', 'opened', 'her', 'laptop', ',', 'brewed', 'some', 'strong', 'coffee', ',', 'and', 'began', 'reading', 'a', 'long', 'article', 'about', 'natural', 'language', 'processing', '.', 'To', 'be', ',', 'or', 'not', 'to', 'be', '?', 'The', 'article', 'explained', 'how', 'tokens', 'are', 'created', ',', 'why', 'lemmatization', 'improves', 'accuracy', ',', 'and', 'when', 'stemming', 'can', 'distort', 'meaning', '.', 'Bombastic', ',', 'fantastic', '!', 'salam', '@', 'balam', '.', 'xalam', 'Excited', 'by', 'these', 'ideas', ',', 'the', 'developer', 'built', 'a', 'small', 'prototype', 'that', 'analyzed', 'movie', 'reviews', ',', 'predicted', 'sentiments', ',', 'and', 'even', 'generated', 'short', 'summaries', '.', 'Although', 'the', 'first', 'results', 'were', 'imperfect', ',', 'she', 'felt', 'motivated', 'to', 'continue', 'experimenting', 'and', 'improving', 'her', 'model', '.']\n",
      "\n",
      " RegexpTokenizer\n",
      "['zamanov@itstep.org', 'salam@balam.xalam']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Лемматизация",
   "id": "ded9bf1c5ccbf9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:33:06.601924Z",
     "start_time": "2025-12-10T16:33:06.592247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"\\n WordLemmatizer\")\n",
    "\n",
    "# v - verb, n - noun, a - adjective, r - adverb\n",
    "print(\"\\n Verbs\")\n",
    "print(f\"running -> {lemmatizer.lemmatize('running', pos='v')}\")\n",
    "print(f\"runs -> {lemmatizer.lemmatize('runs', pos='v')}\")\n",
    "print(f\"runs -> {lemmatizer.lemmatize('ran', pos='v')}\")\n",
    "\n",
    "# adjective\n",
    "print(\"Adjectives\")\n",
    "print(f\"better -> {lemmatizer.lemmatize('better', pos='a')}\")\n",
    "\n",
    "print(\"\\nNouns\")\n",
    "print(f\"cats -> {lemmatizer.lemmatize('cats', pos='n')}\")\n",
    "print(f\"geese -> {lemmatizer.lemmatize('geese', pos='n')}\")\n",
    "print(f\"mice -> {lemmatizer.lemmatize('mice', pos='n')}\")"
   ],
   "id": "2e579c7abcfa55c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " WordLemmatizer\n",
      "\n",
      " Verbs\n",
      "running -> run\n",
      "runs -> run\n",
      "runs -> run\n",
      "Adjectives\n",
      "better -> good\n",
      "\n",
      "Nouns\n",
      "cats -> cat\n",
      "geese -> goose\n",
      "mice -> mouse\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T16:59:16.670675Z",
     "start_time": "2025-12-10T16:59:16.663209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "sentence = \"The cats are running faster than the dogs were walking\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tagged = pos_tag(tokens)\n",
    "# print(pos_tagged)\n",
    "\n",
    "for word, pos in pos_tagged:\n",
    "    wordnet_pos = get_wordnet_pos(word)\n",
    "    lemma = lemmatizer.lemmatize(word.lower(), pos=wordnet_pos)\n",
    "    print(f\"word: {word:12}({pos:4}->{lemma})\")"
   ],
   "id": "d78096d9dd972dc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: The         (DT  ->the)\n",
      "word: cats        (NNS ->cat)\n",
      "word: are         (VBP ->are)\n",
      "word: running     (VBG ->running)\n",
      "word: faster      (RBR ->faster)\n",
      "word: than        (IN  ->than)\n",
      "word: the         (DT  ->the)\n",
      "word: dogs        (NNS ->dog)\n",
      "word: were        (VBD ->were)\n",
      "word: walking     (VBG ->walking)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Стемминг",
   "id": "89171d62ddbbd913"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:11:33.403417Z",
     "start_time": "2025-12-10T17:11:33.395233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import (PorterStemmer, SnowballStemmer, LancasterStemmer)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "words = [\"running\", 'runner', 'easily', 'fairly', 'programming', 'programmer', 'programs']\n",
    "\n",
    "print(f\"{'Word':<15} {'Porter':<12} {'Snowball':<12} {'Lancaster':<12}\")\n",
    "print('-'*50)\n",
    "for word in words:\n",
    "    p = porter.stem(word)\n",
    "    s = snowball.stem(word)\n",
    "    l = lancaster.stem(word)\n",
    "    print(f\"{word:<15} {p:<12} {s:<12} {l:<12}\")"
   ],
   "id": "995128c66c3fc6b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Porter       Snowball     Lancaster   \n",
      "--------------------------------------------------\n",
      "running         run          run          run         \n",
      "runner          runner       runner       run         \n",
      "easily          easili       easili       easy        \n",
      "fairly          fairli       fair         fair        \n",
      "programming     program      program      program     \n",
      "programmer      programm     programm     program     \n",
      "programs        program      program      program     \n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:13:58.095495Z",
     "start_time": "2025-12-10T17:13:58.090362Z"
    }
   },
   "cell_type": "code",
   "source": "print(SnowballStemmer.languages)",
   "id": "be3aa46a86764d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Стоп слова",
   "id": "b86e0e574f519c89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:17:38.969178Z",
     "start_time": "2025-12-10T17:17:38.963602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "print(sorted(list(english_stopwords)))"
   ],
   "id": "bf702277b833b861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:22:55.000088Z",
     "start_time": "2025-12-10T17:22:54.993487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "azerbaijani = set(stopwords.words('azerbaijani'))\n",
    "print(sorted(list(azerbaijani))[:20])\n"
   ],
   "id": "590efcf7986e7d29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ad', 'altmış', 'altı', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax', 'belə', 'beş', 'bilər', 'bir', 'biraz', 'biri', 'birşey', 'biz', 'bizim', 'bizlər']\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:23:20.244870Z",
     "start_time": "2025-12-10T17:23:20.240710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(azerbaijani))\n",
    "print(len(english_stopwords))\n"
   ],
   "id": "3aba5ebf837e37a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "198\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:35:07.200779Z",
     "start_time": "2025-12-10T17:35:07.186217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(txt, use_lemma=True ):\n",
    "    txt = txt.lower()\n",
    "    tokens = word_tokenize(txt)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in  tokens if token.isalpha() and token not in stop_words]\n",
    "    if use_lemma:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        tokens = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "sample_text = \"\"\"The quick brown foxes were running across the field, while the children were watching them from the old wooden fence.\n",
    "One of the kids said that he had gone there yesterday and seen the wolves hunting near the river.\n",
    "They were surprised because the animals usually stay deep in the forest and avoid people.\n",
    "In recent years, scientists have been studying their behavior to understand how climate change affects wildlife.\n",
    "\"\"\"\n",
    "\n",
    "print(preprocess_text(sample_text))\n",
    "print(preprocess_text(sample_text, False))"
   ],
   "id": "6ca7089cd4791b86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'run', 'across', 'field', 'child', 'watch', 'old', 'wooden', 'fence', 'one', 'kid', 'say', 'go', 'yesterday', 'see', 'wolf', 'hunt', 'near', 'river', 'surprise', 'animal', 'usually', 'stay', 'deep', 'forest', 'avoid', 'people', 'recent', 'year', 'scientist', 'study', 'behavior', 'understand', 'climate', 'change', 'affect', 'wildlife']\n",
      "['quick', 'brown', 'fox', 'run', 'across', 'field', 'children', 'watch', 'old', 'wooden', 'fenc', 'one', 'kid', 'said', 'gone', 'yesterday', 'seen', 'wolv', 'hunt', 'near', 'river', 'surpris', 'anim', 'usual', 'stay', 'deep', 'forest', 'avoid', 'peopl', 'recent', 'year', 'scientist', 'studi', 'behavior', 'understand', 'climat', 'chang', 'affect', 'wildlif']\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Корпуса NLTK",
   "id": "bf40561a06568206"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:40:44.759447Z",
     "start_time": "2025-12-10T17:40:44.716774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import brown, gutenberg\n",
    "print(brown.categories())"
   ],
   "id": "14497188a2688a39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\zamanov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:43:34.409194Z",
     "start_time": "2025-12-10T17:43:34.228215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(brown.words(categories='news')[:50])\n",
    "print(len(brown.words(categories='news')))"
   ],
   "id": "d0751dd018be1674",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise']\n",
      "100554\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a6790275f738f20"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
