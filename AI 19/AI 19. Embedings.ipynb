{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.498973Z",
     "start_time": "2025-12-12T16:21:32.494054Z"
    }
   },
   "source": [
    "from zoneinfo import available_timezones\n",
    "\n",
    "import gensim\n",
    "from gensim import downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.541558Z",
     "start_time": "2025-12-12T16:21:32.534762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = ['cat', 'dog', 'bird', 'fish', 'elephant']\n",
    "# One-Hot encoding\n",
    "for i, word in enumerate(vocab):\n",
    "    one_hot = np.zeros(len(vocab), dtype=int)\n",
    "    one_hot[i] = 1\n",
    "    print(f\"{word:10} = {one_hot}\")"
   ],
   "id": "4672301ebbc2e8a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat        = [1 0 0 0 0]\n",
      "dog        = [0 1 0 0 0]\n",
      "bird       = [0 0 1 0 0]\n",
      "fish       = [0 0 0 1 0]\n",
      "elephant   = [0 0 0 0 1]\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.572036Z",
     "start_time": "2025-12-12T16:21:32.564675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = {\n",
    "    'cat': np.array([0.8, 0.2, -0.1, 0.5]),\n",
    "    'dog':np.array([0.7, 0.3, -0.2, 0.6]),\n",
    "    'bird':np.array([0.1, 0.9, 0.8, -0.1]),\n",
    "    'fish':np.array([-0.5, 0.1, -0.3, 0.9]),\n",
    "    'elephant':np.array([0.6, 0.4, -0.3, 0.4])\n",
    "}\n",
    "\n",
    "for word, vec in enumerate(embeddings.items()):\n",
    "    print(f\"{word:10} = {vec}\")"
   ],
   "id": "42c5da5d6d8aa27c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0 = ('cat', array([ 0.8,  0.2, -0.1,  0.5]))\n",
      "         1 = ('dog', array([ 0.7,  0.3, -0.2,  0.6]))\n",
      "         2 = ('bird', array([ 0.1,  0.9,  0.8, -0.1]))\n",
      "         3 = ('fish', array([-0.5,  0.1, -0.3,  0.9]))\n",
      "         4 = ('elephant', array([ 0.6,  0.4, -0.3,  0.4]))\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.612202Z",
     "start_time": "2025-12-12T16:21:32.604217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "words = list(embeddings.keys())\n",
    "for i in range(len(words)):\n",
    "    for j in range(i+1, len(words)):\n",
    "        word1, word2 = words[i], words[j]\n",
    "        sim = cosine_similarity(embeddings[word1], embeddings[word2])\n",
    "        print(f\"{word1:10}: {word2:10}: {sim:.3f}\")\n"
   ],
   "id": "320d5e6013e4785e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat       : dog       : 0.979\n",
      "cat       : bird      : 0.111\n",
      "cat       : fish      : 0.096\n",
      "cat       : elephant  : 0.929\n",
      "dog       : bird      : 0.100\n",
      "dog       : fish      : 0.263\n",
      "dog       : elephant  : 0.967\n",
      "bird      : fish      : -0.222\n",
      "bird      : elephant  : 0.132\n",
      "fish      : elephant  : 0.201\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.631755Z",
     "start_time": "2025-12-12T16:21:32.624715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [\n",
    "    ['cat', 'sits'],\n",
    "    ['cat', 'sits', 'on', 'soft', 'sofa'],\n",
    "    ['small', 'dog', 'runs', 'fast'],\n",
    "    ['dog', 'lies', 'under', 'the', 'table'],\n",
    "    ['child', 'plays', 'with', 'a', 'red', 'ball'],\n",
    "    ['man', 'reads'],\n",
    "    ['man', 'reads', 'interesting', 'book', 'at', 'home'],\n",
    "    ['woman', 'drinks', 'hot', 'coffee'],\n",
    "    ['sun', 'shines'],\n",
    "    ['bright', 'sun', 'shines', 'in', 'blue', 'sky'],\n",
    "    ['rain', 'falls', 'on', 'wet', 'street'],\n",
    "    ['bird', 'flies', 'high'],\n",
    "    ['bird', 'flies', 'over', 'green', 'trees'],\n",
    "    ['developer', 'writes', 'clean', 'code'],\n",
    "    ['experienced', 'developer', 'writes', 'efficient', 'code', 'quickly'],\n",
    "    ['students', 'learn', 'programming'],\n",
    "    ['fat','students', 'learn', 'modern', 'programming', 'technologies'],\n",
    "    ['neural', 'network', 'learns', 'from', 'data'],\n",
    "    ['machine', 'learning', 'model', 'improves', 'with', 'more', 'data']\n",
    "]\n",
    "\n",
    "print(f\"corpus length: {len(corpus)}\")\n",
    "\n",
    "all_words = set()\n",
    "for sentence in corpus:\n",
    "    all_words.update(sentence)\n",
    "print(f\"Unique words: {len(all_words)}\")"
   ],
   "id": "797b5a09149b93fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 19\n",
      "Unique words: 68\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.939080Z",
     "start_time": "2025-12-12T16:21:32.654338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=50,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    epochs=100,\n",
    "    sg=1)\n",
    "\n",
    "print(\"Model Word2Vec learned\")\n",
    "print(f\"Word length: {model.wv.vector_size}\")\n",
    "print(\"Words in dictionary\")\n",
    "print(list(model.wv.key_to_index.keys()))"
   ],
   "id": "d6067b6e6ad3ff3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Word2Vec learned\n",
      "Word length: 50\n",
      "Words in dictionary\n",
      "['data', 'programming', 'learn', 'students', 'code', 'writes', 'developer', 'flies', 'bird', 'shines', 'sun', 'reads', 'man', 'with', 'dog', 'on', 'sits', 'cat', 'more', 'improves', 'model', 'learning', 'machine', 'from', 'learns', 'network', 'neural', 'technologies', 'modern', 'fat', 'quickly', 'efficient', 'experienced', 'clean', 'trees', 'green', 'over', 'high', 'street', 'wet', 'falls', 'rain', 'sky', 'blue', 'in', 'bright', 'coffee', 'hot', 'drinks', 'woman', 'home', 'at', 'book', 'interesting', 'ball', 'red', 'a', 'plays', 'child', 'table', 'the', 'under', 'lies', 'fast', 'runs', 'small', 'sofa', 'soft']\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.959218Z",
     "start_time": "2025-12-12T16:21:32.952885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cat_vector = model.wv['cat']\n",
    "print(f\"Cat vector: {cat_vector}\")"
   ],
   "id": "9e6ea6c574424f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat vector: [-1.46846636e-04  7.13470727e-05  4.47533348e-05 -8.88036843e-03\n",
      "  8.44077487e-03 -4.59163636e-03  5.15610306e-03  2.68266699e-03\n",
      "  1.12458318e-02 -1.43130552e-02 -1.34174451e-02 -1.13731558e-02\n",
      "  2.04127971e-02 -1.50801823e-03 -1.95932556e-02  3.25216446e-04\n",
      " -8.60978011e-03  1.18914098e-02 -2.06715800e-02  4.05027252e-03\n",
      " -1.85145214e-02  3.50792520e-03  1.35161625e-02  1.46568203e-02\n",
      " -1.47985993e-02 -1.19349500e-02 -1.39158070e-02 -1.47937667e-02\n",
      " -1.93910040e-02 -3.80832003e-03 -1.88230595e-03 -1.60194617e-02\n",
      "  1.32786566e-02  1.98642188e-03  1.09200906e-02  3.71146179e-03\n",
      "  3.12428805e-03 -1.47172408e-02 -3.73650901e-03  9.47283302e-03\n",
      " -9.68759786e-03  1.49215397e-03  5.38492249e-03 -2.55165552e-03\n",
      "  2.13243738e-02  1.57046895e-02  4.54508141e-03  1.33171640e-02\n",
      "  1.25190737e-02 -1.05128419e-02]\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:32.987724Z",
     "start_time": "2025-12-12T16:21:32.981652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dog_vector = model.wv['dog']\n",
    "print(f\"Dog vector: {dog_vector}\")"
   ],
   "id": "1a4d4ca879ba11b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog vector: [ 1.6483413e-02 -1.0087132e-02  1.8561190e-02  1.8135976e-02\n",
      " -9.5770033e-03 -2.4265570e-05  9.9850949e-03 -6.0265083e-03\n",
      " -1.2546920e-02 -1.3663703e-02 -1.4045971e-03 -4.7742962e-03\n",
      "  1.2493265e-02 -2.0530559e-03 -1.8440785e-03  6.8224138e-03\n",
      "  1.0769965e-02 -9.2145882e-04 -1.4435704e-02 -2.1683894e-02\n",
      "  1.3141345e-03  1.5437750e-02  5.5774637e-03 -1.8423598e-02\n",
      " -1.4611667e-02  1.3526761e-02 -8.0064954e-03  1.5274231e-02\n",
      " -1.3783852e-02  1.8315295e-02 -1.2848108e-02  4.4922573e-03\n",
      " -2.8530618e-03 -1.4317548e-02 -8.2964795e-03 -1.7386135e-03\n",
      " -8.3751893e-03 -3.0055547e-03 -1.4327246e-02  7.1145212e-03\n",
      "  1.9171722e-02 -5.4647708e-03 -1.8665207e-03  6.9338782e-03\n",
      "  2.0593235e-02 -1.3323303e-02 -1.4170116e-02 -7.9342574e-03\n",
      "  1.9580152e-02  2.6528244e-03]\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:33.019151Z",
     "start_time": "2025-12-12T16:21:33.012436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similar_to_cat = model.wv.most_similar('cat', topn=5)\n",
    "for word, similarity in similar_to_cat:\n",
    "    print(f\"{word:10}: {similarity:.3f}\")"
   ],
   "id": "c27560519e89141c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red       : 0.396\n",
      "small     : 0.387\n",
      "drinks    : 0.327\n",
      "writes    : 0.291\n",
      "network   : 0.290\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:33.062190Z",
     "start_time": "2025-12-12T16:21:33.057459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similar_to_dog = model.wv.most_similar('dog', topn=5)\n",
    "for word, similarity in similar_to_dog:\n",
    "    print(f\"{word:10}: {similarity:.3f}\")"
   ],
   "id": "3a23938151a7029e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reads     : 0.562\n",
      "street    : 0.460\n",
      "from      : 0.440\n",
      "bright    : 0.328\n",
      "rain      : 0.313\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:33.085143Z",
     "start_time": "2025-12-12T16:21:33.079245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sim_cat_dog = model.wv.similarity('cat', 'dog')\n",
    "print(sim_cat_dog)"
   ],
   "id": "356a404e20911768",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04154672\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:33.111796Z",
     "start_time": "2025-12-12T16:21:33.106813Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.wv.similarity('code', 'developer'))",
   "id": "3670a6ac793b5da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021909581\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:21:33.506870Z",
     "start_time": "2025-12-12T16:21:33.134109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "available_models = api.info()['models']\n",
    "for name, info in list(available_models.items())[:10]:\n",
    "    size_mb = info.get('file_size', 0)/(1024*1024)\n",
    "\n",
    "    print(f\"{name:10}: {info} ({size_mb:.2f} MB)\")"
   ],
   "id": "125be79df6a85b58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300: {'num_records': 999999, 'file_size': 1005007116, 'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py', 'license': 'https://creativecommons.org/licenses/by-sa/3.0/', 'parameters': {'dimension': 300}, 'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).', 'read_more': ['https://fasttext.cc/docs/en/english-vectors.html', 'https://arxiv.org/abs/1712.09405', 'https://arxiv.org/abs/1607.01759'], 'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af', 'file_name': 'fasttext-wiki-news-subwords-300.gz', 'parts': 1} (958.45 MB)\n",
      "conceptnet-numberbatch-17-06-300: {'num_records': 1917247, 'file_size': 1225497562, 'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py', 'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt', 'parameters': {'dimension': 300}, 'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.', 'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972', 'https://github.com/commonsense/conceptnet-numberbatch', 'http://conceptnet.io/'], 'checksum': 'fd642d457adcd0ea94da0cd21b150847', 'file_name': 'conceptnet-numberbatch-17-06-300.gz', 'parts': 1} (1168.73 MB)\n",
      "word2vec-ruscorpora-300: {'num_records': 184973, 'file_size': 208427381, 'base_dataset': 'Russian National Corpus (about 250M words)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py', 'license': 'https://creativecommons.org/licenses/by/4.0/deed.en', 'parameters': {'dimension': 300, 'window_size': 10}, 'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.', 'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS', 'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models', 'http://rusvectores.org/en/', 'https://github.com/RaRe-Technologies/gensim-data/issues/3'], 'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4', 'file_name': 'word2vec-ruscorpora-300.gz', 'parts': 1} (198.77 MB)\n",
      "word2vec-google-news-300: {'num_records': 3000000, 'file_size': 1743563840, 'base_dataset': 'Google News (about 100 billion words)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py', 'license': 'not found', 'parameters': {'dimension': 300}, 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\", 'read_more': ['https://code.google.com/archive/p/word2vec/', 'https://arxiv.org/abs/1301.3781', 'https://arxiv.org/abs/1310.4546', 'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'], 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef', 'file_name': 'word2vec-google-news-300.gz', 'parts': 1} (1662.79 MB)\n",
      "glove-wiki-gigaword-50: {'num_records': 400000, 'file_size': 69182535, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 50}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813', 'file_name': 'glove-wiki-gigaword-50.gz', 'parts': 1} (65.98 MB)\n",
      "glove-wiki-gigaword-100: {'num_records': 400000, 'file_size': 134300434, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 100}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '40ec481866001177b8cd4cb0df92924f', 'file_name': 'glove-wiki-gigaword-100.gz', 'parts': 1} (128.08 MB)\n",
      "glove-wiki-gigaword-200: {'num_records': 400000, 'file_size': 264336934, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 200}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '59652db361b7a87ee73834a6c391dfc1', 'file_name': 'glove-wiki-gigaword-200.gz', 'parts': 1} (252.09 MB)\n",
      "glove-wiki-gigaword-300: {'num_records': 400000, 'file_size': 394362229, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 300}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '29e9329ac2241937d55b852e8284e89b', 'file_name': 'glove-wiki-gigaword-300.gz', 'parts': 1} (376.09 MB)\n",
      "glove-twitter-25: {'num_records': 1193514, 'file_size': 109885004, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 25}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '50db0211d7e7a2dcd362c6b774762793', 'file_name': 'glove-twitter-25.gz', 'parts': 1} (104.79 MB)\n",
      "glove-twitter-50: {'num_records': 1193514, 'file_size': 209216938, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 50}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'c168f18641f8c8a00fe30984c4799b2b', 'file_name': 'glove-twitter-50.gz', 'parts': 1} (199.52 MB)\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:22:12.310291Z",
     "start_time": "2025-12-12T16:21:33.542178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_model = api.load('glove-twitter-25')\n",
    "print('model loaded')\n",
    "print(f\"dictionary size: {len(glove_model)}\")\n",
    "print(f\"vector size: {glove_model.vector_size}\")"
   ],
   "id": "ab9bdf20dcb8c93b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "dictionary size: 1193514\n",
      "vector size: 25\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:22:12.435131Z",
     "start_time": "2025-12-12T16:22:12.315768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word, similarity in glove_model.most_similar('king', topn=10):\n",
    "    print(f\"{word:10}: {similarity}\")\n",
    "print()\n",
    "for word, similarity in glove_model.most_similar('computer', topn=10):\n",
    "    print(f\"{word:10}: {similarity}\")"
   ],
   "id": "ea907a3feffb84d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prince    : 0.9337409734725952\n",
      "queen     : 0.9202421307563782\n",
      "aka       : 0.9176921844482422\n",
      "lady      : 0.9163240790367126\n",
      "jack      : 0.9147354364395142\n",
      "'s        : 0.9066898226737976\n",
      "stone     : 0.8982374668121338\n",
      "mr.       : 0.8919409513473511\n",
      "the       : 0.889343798160553\n",
      "star      : 0.8892088532447815\n",
      "\n",
      "camera    : 0.907833456993103\n",
      "cell      : 0.891890287399292\n",
      "server    : 0.874466598033905\n",
      "device    : 0.8693525195121765\n",
      "wifi      : 0.863125741481781\n",
      "screen    : 0.8621907234191895\n",
      "app       : 0.8615543246269226\n",
      "case      : 0.8587921857833862\n",
      "remote    : 0.8583616018295288\n",
      "file      : 0.8575270771980286\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:54:49.216448Z",
     "start_time": "2025-12-12T16:54:49.176881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# king - man + woman = ?\n",
    "result = glove_model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=5)\n",
    "for word, similarity in result:\n",
    "    print(f\"{word:10}: {similarity}\")\n"
   ],
   "id": "b0380bd30f9e1786",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meets     : 0.8841923475265503\n",
      "prince    : 0.832163393497467\n",
      "queen     : 0.8257461190223694\n",
      "â€™s        : 0.8174097537994385\n",
      "crow      : 0.813499391078949\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:54:50.035088Z",
     "start_time": "2025-12-12T16:54:49.995946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# paris - france + russia = ?\n",
    "result2 = glove_model.most_similar(positive=[\"paris\", \"russia\"], negative=[\"france\"], topn=5)\n",
    "for word, similarity in result2:\n",
    "    print(f\"{word:10}: {similarity}\")"
   ],
   "id": "722e8d92682583ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brazil    : 0.8781961798667908\n",
      "italy     : 0.8547845482826233\n",
      "australia : 0.8511278033256531\n",
      "city      : 0.8145042061805725\n",
      "hawaii    : 0.8138893246650696\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T16:54:50.879074Z",
     "start_time": "2025-12-12T16:54:50.838717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result3 = glove_model.most_similar(positive=[\"fast\", \"bad\"], negative=[\"slow\"], topn=5)\n",
    "for word, similarity in result3:\n",
    "    print(f\"{word:10}: {similarity}\")"
   ],
   "id": "9c5a163726ab97ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yet       : 0.8893000483512878\n",
      "also      : 0.8887512683868408\n",
      "made      : 0.879071056842804\n",
      "'d        : 0.877172589302063\n",
      "want      : 0.8767794370651245\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed7b8f26598c138b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
